{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessarry dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from auxFonctions import AminoAcid\n",
    "import fonctionsSupervisedLearning2 as fsl2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "\n",
    "from Bio.Align import substitution_matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redoing fast the Statistical Study in order to get the s values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a file into a list of entries\n",
    "with open('data/SIG_13.red', 'r') as file:\n",
    "    entries = file.read().split('\\n   ')\n",
    "\n",
    "\n",
    "# Define a function to process each entry in the data file\n",
    "def process_entry(entry):\n",
    "    lines = entry.split('\\n')\n",
    "    protein_id, primary_structure, annotation = lines\n",
    "    return {\n",
    "        'Protein ID': protein_id.split()[1],\n",
    "        'Primary Structure': primary_structure,\n",
    "        'Annotation': annotation\n",
    "    }\n",
    "\n",
    "\n",
    "# Process each entry\n",
    "processed_entries = [process_entry(entry) for entry in entries]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(processed_entries)\n",
    "df['Cleavage_Site'] = df['Annotation'].apply(lambda x: x.find('C'))\n",
    "\n",
    "# Now you can analyze the DataFrame as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average position of the cleavage site:\n",
      "24.04971590909091\n",
      "\n",
      "\n",
      "The extremum position of the cleavage site:\n",
      "13\n",
      "90\n",
      "The distance from cleavage site to the C-terminal part of the amino-sequence:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the position of the cleavage site\n",
    "cleavage_site_position = df['Cleavage_Site']\n",
    "#print(\"Position of the cleavage site:\")\n",
    "#print(cleavage_site_position)\n",
    "print(\"Average position of the cleavage site:\")\n",
    "print(cleavage_site_position.mean())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The extremum position of the cleavage site:\")\n",
    "print(cleavage_site_position.min())\n",
    "print(cleavage_site_position.max())\n",
    "\n",
    "print(\"The distance from cleavage site to the C-terminal part of the amino-sequence:\")\n",
    "#there is always 30 amino-acids after the cleavage site\n",
    "print(\"\\n\")\\\n",
    "\n",
    "# with have then p = [13, 1] and q = [1, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the primary structure into a list of amino acids\n",
    "amino_acid_seq = df['Primary Structure'].apply(lambda x: list(x))\n",
    "\n",
    "# Create a DataFrame to store, for each primary structure, the neihborhood of the cleavage site\n",
    "# The neighborhood is defined as the word of length p+q starting p letters before the cleavage site\n",
    "correct_neighborhood = pd.Series(index=amino_acid_seq.index, dtype=str)\n",
    "for i, seq in amino_acid_seq.items():\n",
    "    correct_neighborhood[i] = ''.join(seq[cleavage_site_position[i]-13:cleavage_site_position[i]+2])\n",
    "\n",
    "# for each amino acid in the sequence, replace it with the corresponding AminoAcid object\n",
    "amino_acid_seqB = amino_acid_seq\n",
    "amino_acid_seq = amino_acid_seq.apply(lambda x: [AminoAcid(aa) for aa in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametres de l'étude\n",
    "p = 13\n",
    "q = 2\n",
    "\n",
    "# Create a DataFrame to store the counts of each amino acid at every position relative to the cleavage site\n",
    "#the cleavage site is between to aminoacids, so cleavage_site_position is the position of the first amino acid after the cleavge site\n",
    "#So i need to create a dataframe with columns from -p to q without 0\n",
    "amino_acid_counts = pd.DataFrame(0, index=AminoAcid.properties.keys(), columns=range(-p, q))\n",
    "amino_acid_freqs = pd.DataFrame(0.0, index=AminoAcid.properties.keys(), columns=range(-p, q)) #f(a,i)\n",
    "amino_acid_pseudo_counts = pd.DataFrame(0, index=AminoAcid.properties.keys(), columns=range(-p, q)) #g(a)\n",
    "amino_acid_s_values = pd.DataFrame(0.0, index=AminoAcid.properties.keys(), columns=range(-p, q)) #s(a,i)\n",
    "\n",
    "\n",
    "# Count the occurrences of each amino acid at every position relative to the cleavage site\n",
    "\n",
    "for i, seq in amino_acid_seq.items():\n",
    "    for j, aa in enumerate(seq):\n",
    "        position = j - cleavage_site_position[i] #position of the amino acid relative to the cleavage site\n",
    "        if position in amino_acid_counts.columns:\n",
    "            amino_acid_counts.loc[aa.code, position] += 1\n",
    "\n",
    "# Add pseudo-counts to avoid zero counts here pseudocount parameter is 1/len(df)\n",
    "amino_acid_pseudo_counts = amino_acid_counts + 1\n",
    "\n",
    "# Print the results\n",
    "#print(\"Occurrences of each amino acid at every position relative to the cleavage site:\")\n",
    "#print(amino_acid_pseudo_counts)\n",
    "\n",
    "# Compute the observed frequency of each amino acid at the relative position (using pseudo-counts)\n",
    "for i in amino_acid_counts.index:\n",
    "    for j in amino_acid_counts.columns:\n",
    "        amino_acid_freqs.loc[i, j] = amino_acid_pseudo_counts.loc[i, j] / len(df)\n",
    "\n",
    "# Compute the general background frequency of each amino acid\n",
    "general_background_frequency = amino_acid_freqs.mean(axis=1)\n",
    "\n",
    "# Compute the s value of each amino acid at every position\n",
    "for i in amino_acid_counts.index:\n",
    "    for j in amino_acid_counts.columns:\n",
    "        amino_acid_s_values.loc[i, j] = math.log(amino_acid_freqs.loc[i, j]) - math.log(general_background_frequency[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store, for each primary structure, the neihborhood of the cleavage site\n",
    "# The neighborhood is defined as the word of length p+q starting p letters before the cleavage site\n",
    "correct_neighborhood = pd.Series()\n",
    "for i, seq in amino_acid_seqB.items():\n",
    "    correct_neighborhood[i] = ''.join(seq[cleavage_site_position[i]-p:cleavage_site_position[i]+q])\n",
    "\n",
    "# Create a DataFrame to store, for each primary structure, a sequence that is not the neighborhood of the cleavage site\n",
    "incorrect_neighborhood = pd.Series()\n",
    "decalage  = [1,2,3,4,5, -1,-2,-3,-4, -5]\n",
    "for i, seq in amino_acid_seqB.items():\n",
    "    dec = np.random.choice(decalage)\n",
    "    dec = 0 if cleavage_site_position[i]-13 - dec < 0 else dec\n",
    "    incorrect_neighborhood[i] = ''.join(seq[cleavage_site_position[i]-p-dec:cleavage_site_position[i]+q-dec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.302718213229582"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function computing the q-1 score for a given word\n",
    "def q_minus_1_score(word):\n",
    "    return sum([amino_acid_s_values.loc[aa, i-p] for i, aa in enumerate(word)])\n",
    "\n",
    "\n",
    "#A REDEFINIR EN FONCTION DES RESULTATS OBTENUS\n",
    "threshold = 1.5\n",
    "print(threshold)\n",
    "\n",
    "#A simple thresholding (to be tuned) is then enough to define a simple binary classifier.\n",
    "def is_cleavage_neighborhood(score):\n",
    "    return score > threshold\n",
    "\n",
    "q_minus_1_score('AAAAAAAAAAAAAAA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score of the correct neighborhoods:\n",
      "3.2162682210182796\n",
      "\n",
      "\n",
      "Mean score of the incorrect neighborhoods:\n",
      "-1.365212992873393\n",
      "\n",
      "\n",
      "Standard deviation of the score of the correct neighborhoods:\n",
      "2.5872637135991514\n",
      "Standard deviation of the score of the incorrect neighborhoods:\n",
      "2.660060004379069\n",
      "Min and max values of the correct neighboorhoods score :\n",
      "-7.931205228611351\n",
      "9.452566054567916\n",
      "Min and max values of the incorrect neighboorhoods score :\n",
      "-13.760389553652788\n",
      "7.465159091616986\n",
      "False negative out of 1408 entries:\n",
      "288\n",
      "False positive out of 1408 entries:\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "# To obtain the score of the correct neighborhoods, we apply the q-1 score function to each neighborhood\n",
    "#correct_neighborhood = correct_neighborhood.apply(lambda x: [AminoAcid(aa) for aa in x])\n",
    "correct_neigboorhood_score = correct_neighborhood.apply(q_minus_1_score)\n",
    "incorrect_neighborhood_score = incorrect_neighborhood.apply(q_minus_1_score)\n",
    "\n",
    "#print(\"Score of the correct neighborhoods:\")\n",
    "#print(correct_neigboorhood_score)\n",
    "#print(\"\\n\")\n",
    "\n",
    "print(\"Mean score of the correct neighborhoods:\")\n",
    "print(correct_neigboorhood_score.mean())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Mean score of the incorrect neighborhoods:\")\n",
    "print(incorrect_neighborhood_score.mean())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Standard deviation of the score of the correct neighborhoods:\")\n",
    "print(correct_neigboorhood_score.std())\n",
    "\n",
    "print(\"Standard deviation of the score of the incorrect neighborhoods:\")\n",
    "print(incorrect_neighborhood_score.std())\n",
    "\n",
    "print(\"Min and max values of the correct neighboorhoods score :\")\n",
    "print(correct_neigboorhood_score.min())\n",
    "print(correct_neigboorhood_score.max())\n",
    "\n",
    "print(\"Min and max values of the incorrect neighboorhoods score :\")\n",
    "print(incorrect_neighborhood_score.min())\n",
    "print(incorrect_neighborhood_score.max())\n",
    "\n",
    "#We now test, with the updated threshold, the performance of the classifier on the training set\n",
    "# Treshold = mean - std\n",
    "false_negatives = correct_neigboorhood_score[correct_neigboorhood_score < threshold].count()\n",
    "print(\"False negative out of \"+str(len(correct_neighborhood))+ \" entries:\")\n",
    "print(false_negatives)\n",
    "\n",
    "false_positives = incorrect_neighborhood_score[incorrect_neighborhood_score > threshold].count()\n",
    "print(\"False positive out of \"+str(len(incorrect_neighborhood))+ \" entries:\")\n",
    "print(false_positives)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILISATION DES RESULTATS POUR CREER LA KERNEL PROBABILISTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_un_caractere(obj):\n",
    "    return isinstance(obj, str) and len(obj) == 1\n",
    "\n",
    "def Phi(x : chr, y : chr, i : int) :\n",
    "    if not(est_un_caractere(x) and est_un_caractere(y)) :\n",
    "        raise ValueError(\"x and y must be single characters\")\n",
    "    '''\n",
    "    Fonction servant de base a la Kernel probabiliste\n",
    "    ### Parameters:\n",
    "    - x : un acide aminé (sous forme de string)\n",
    "    - y : un acide aminé (sous forme de string)\n",
    "    - i : un entier compris entre -p et q-1 (inclus)\n",
    "    ### Returns:\n",
    "    - La valeur de la fonction Phi_i(x,y)\n",
    "    '''\n",
    "    if (x == y) :\n",
    "        return ( amino_acid_s_values.loc[x, i] + math.log(1 + math.exp(amino_acid_s_values.loc[x, i])) )\n",
    "    else :\n",
    "        return ( amino_acid_s_values.loc[x, i] + amino_acid_s_values.loc[y, i] )\n",
    "\n",
    "def LogKernel(x : str, y : str) :\n",
    "    '''\n",
    "    Fonction servant de base a la Kernel probabiliste\n",
    "    ### Parameters:\n",
    "    - x : une sequence d'acides aminés de taille p+q (sous forme de string)\n",
    "    - y : une sequence d'acides aminés de taille p+q (sous forme de string)\n",
    "    ### Returns:\n",
    "    - La valeur de la fonction LogKernel(x,y)\n",
    "    '''\n",
    "    sum = 0\n",
    "    for i in range(-p, q) :\n",
    "        sum += Phi(x[p+i], y[p+i], i)\n",
    "        #print(\"Sum :\"+ str(sum))\n",
    "    return sum\n",
    "\n",
    "\"\"\"\n",
    "def ProbalisticKernel(X, Y):\n",
    "    # Initialize an empty matrix to store the kernel values\n",
    "    gram_matrix = np.zeros((X.shape[0], Y.shape[0]))\n",
    "\n",
    "    # Calculate the kernel value for each pair of samples\n",
    "    for i, x in enumerate(X):\n",
    "        for j, y in enumerate(Y):\n",
    "            x_str = fsl2.vector_to_word(x)\n",
    "            y_str = fsl2.vector_to_word(y)\n",
    "            gram_matrix[i, j] = math.exp(LogKernel(x_str, y_str))\n",
    "\n",
    "    return gram_matrix\n",
    "\"\"\"\n",
    "\n",
    "def ProbKernel(x, y):\n",
    "    X_str = fsl2.vector_to_word(x)\n",
    "    Y_str = fsl2.vector_to_word(y)\n",
    "    '''\n",
    "    Fonction servant de base a la Kernel probabiliste\n",
    "    ### Parameters:\n",
    "    - x : une sequence d'acides aminés converti au prealable en vecteur de taille (p+q)*26 composé de 0 et 1\n",
    "    - y : une sequence d'acides aminés converti au prealable en vecteur de taille (p+q)*26 composé de 0 et 1\n",
    "    ### Returns:\n",
    "    - La valeur de la fonction Kernel(x,y)\n",
    "    '''\n",
    "    return math.exp(LogKernel(X_str, Y_str))\n",
    "\n",
    "def ProbabilisticKernel(X, Y):\n",
    "    # Initialize an empty matrix to store the kernel values\n",
    "    gram_matrix = np.zeros((X.shape[0], Y.shape[0]))\n",
    "\n",
    "    # Calculate the kernel value for each pair of samples\n",
    "    for i, x in enumerate(X):\n",
    "        for j, y in enumerate(Y):\n",
    "            gram_matrix[i, j] = ProbKernel(x, y)\n",
    "\n",
    "    return gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9846.188567738474\n"
     ]
    }
   ],
   "source": [
    "x = fsl2.word_to_vector(correct_neighborhood[0])\n",
    "y = fsl2.word_to_vector(correct_neighborhood[1])\n",
    "\n",
    "print(ProbKernel(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERNEL ISSUE DES MATRICES DE SUBSTITUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for A and R: -1.0\n",
      "Score for A and A: 2.0\n"
     ]
    }
   ],
   "source": [
    "pam250 = substitution_matrices.load(\"PAM250\")\n",
    "\n",
    "# Access scores directly\n",
    "score_AR = pam250[('A', 'K')]\n",
    "print(\"Score for A and R:\", score_AR)\n",
    "\n",
    "score_AA = pam250[('A', 'A')]\n",
    "print(\"Score for A and A:\", score_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction indiquant la similarité entre deux sequences d'acides aminés\n",
    "def Similarity(x : str, y : str) :\n",
    "    '''\n",
    "    Fonction servant de base a la Kernel probabiliste\n",
    "    ### Parameters:\n",
    "    - x : une sequence d'acides aminés de taille p+q (sous forme de string)\n",
    "    - y : une sequence d'acides aminés de taille p+q (sous forme de string)\n",
    "    ### Returns:\n",
    "    - La valeur de la fonction Similarity(x,y)\n",
    "    '''\n",
    "    sum = 0\n",
    "    for i in range(-p, q) :\n",
    "        sum += pam250[(x[p+i], y[p+i])]\n",
    "    return sum\n",
    "\n",
    "def RBF_similarity(x : str, y : str, sigma = 1) :\n",
    "    '''\n",
    "    Fonction servant de base a la Kernel de similarité\n",
    "    ### Parameters:\n",
    "    - x : une sequence d'acides aminés de taille p+q (sous forme de string)\n",
    "    - y : une sequence d'acides aminés de taille p+q (sous forme de string)\n",
    "    - sigma : un reel positif\n",
    "    ### Returns:\n",
    "    - La valeur de la fonction Similarity(x,y)\n",
    "    '''\n",
    "    return math.exp(-Similarity(x, y) / (2 * sigma**2))\n",
    "\n",
    "def RBF_kernel(X, Y) :\n",
    "    '''\n",
    "    Fonction servant de base a la Kernel RBF (Radial Basis Function) par similarité\n",
    "    ### Parameters:\n",
    "    - x : une sequence d'acides aminés converti au prealable en vecteur de taille (p+q)*26 composé de 0 et 1\n",
    "    - y : une sequence d'acides aminés converti au prealable en vecteur de taille (p+q)*26 composé de 0 et 1\n",
    "    ### Returns:\n",
    "    - La valeur de la fonction Kernel(x,y)\n",
    "    '''\n",
    "    # Initialize an empty matrix to store the kernel values\n",
    "    gram_matrix = np.zeros((X.shape[0], Y.shape[0]))\n",
    "\n",
    "    # Calculate the kernel value for each pair of samples\n",
    "    for i, x in enumerate(X):\n",
    "        X_str = fsl2.vector_to_word(x)\n",
    "        for j, y in enumerate(Y):\n",
    "            Y_str = fsl2.vector_to_word(y)\n",
    "            gram_matrix[i, j] = RBF_similarity(X_str, Y_str)\n",
    "\n",
    "    return gram_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploitable = fsl2.convert_df_to_vectors2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_subseq(row, n:int, nb_letters:int=26):\n",
    "    '''\n",
    "    Extract a random subsequence of length n from the primary structure and the annotation\n",
    "    ### Parameters:\n",
    "    - row: a row of the dataframe\n",
    "    - n: the length of the subsequence\n",
    "    - nb_letters: the number of letters in the alphabet\n",
    "    ### Returns:\n",
    "    - a pandas series containing the subsequence of the primary structure\n",
    "    -There should be as much valid sequences as invalid sequences\n",
    "    the subsequence of the annotation, the subsequence of the primary structure as a vector and \n",
    "    the position of the cleavage site in the subsequence\n",
    "    '''\n",
    "    bool_cleavage = False\n",
    "    random_double = np.random.random()\n",
    "    if random_double > 0.5:\n",
    "        bool_cleavage = True\n",
    "\n",
    "    if bool_cleavage:\n",
    "        start_index = row['Cleavage_Site'] - 13\n",
    "        end_index = start_index + n  #n = 13 + 2 = 15\n",
    "\n",
    "        neighborhood_check = True  # Define wheter the sequence if the right neighborhood of the cleavage site\n",
    "    else :\n",
    "        max_start_index = max(0, len(row['Primary Structure']) - n)  # Calculate the maximum possible start index\n",
    "        if max_start_index == 0:\n",
    "            start_index = 0  # if chain is too short, start at the beginning\n",
    "        else:\n",
    "            start_index = np.random.randint(0, max_start_index)  # Randomly select a start index\n",
    "        end_index = start_index + n  # Calculer l'indice de fin\n",
    "\n",
    "        neighborhood_check = True if (row['Cleavage_Site'] - start_index == 13) else False  # Define wheter the sequence if the right neighborhood of the cleavage site\n",
    "    \n",
    "    return pd.Series([row['Primary Structure'][start_index:end_index], row['P_Structure_vector'][start_index*nb_letters:end_index*nb_letters], neighborhood_check], index=['Primary Structure', 'P_Structure_vector', 'Neighborhood_bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split_random_pos_proba(df, n ,test_size=0.2, random_state=42):\n",
    "    '''\n",
    "    Split the data into training and testing sets\n",
    "    ### Parameters:\n",
    "    - df: the dataframe containing the data\n",
    "    - n: the length of the subsequence\n",
    "    - test_size: the proportion of the data to include in the test split\n",
    "    - random_state: the seed for the random number generator\n",
    "    ### Returns:\n",
    "    - X_train: the training set\n",
    "    - X_test: the testing set\n",
    "    - pos_train: the position of the cleavage site in the training set\n",
    "    - pos_test: the position of the cleavage site in the testing set\n",
    "    '''\n",
    "    df_random = df.apply(extract_random_subseq, axis=1, n=n)\n",
    "    X = np.array(df_random['P_Structure_vector'].tolist())\n",
    "    y = np.array(df_random['Neighborhood_bool'].tolist())   \n",
    "\n",
    "    X_train, X_test, bool_train, bool_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \"\"\"\n",
    "    test_size=0.2: This argument specifies the proportion of the dataset to include in the test split. \n",
    "    In this case, 20% of the data will be used for testing, and the remaining 80% will be used for training\n",
    "\n",
    "    random_state=42: This argument sets the seed for the random number generator that shuffles the data before splitting. \n",
    "    Setting a specific seed (like 42 in this case) ensures that the output is reproducible, i.e., \n",
    "    you'll get the same train/test split each time you run the code.\n",
    "    \"\"\"\n",
    "\n",
    "    return X_train, X_test, bool_train, bool_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "p, q = 13, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developpement et entrainement du modèle probabiliste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1126, 390)\n",
      "\n",
      "\n",
      "569\n",
      "1126\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, bool_train, bool_test = test_train_split_random_pos_proba(df_exploitable, n)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "count_true = 0\n",
    "count_all = 0\n",
    "for t in bool_train:\n",
    "    count_all += 1\n",
    "    if t:\n",
    "        count_true += 1\n",
    "print(\"\\n\")\n",
    "print(count_true)\n",
    "print(count_all)\n",
    "\n",
    "gram_matrix = ProbabilisticKernel(X_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7624113475177305\n"
     ]
    }
   ],
   "source": [
    "probabilisticClassifier = svm.SVC(kernel='precomputed')\n",
    "\n",
    "probabilisticClassifier.fit(gram_matrix, bool_train)\n",
    "\n",
    "gram_matrix_test = ProbabilisticKernel(X_test, X_train)\n",
    "\n",
    "bool_pred = probabilisticClassifier.predict(gram_matrix_test)\n",
    "\n",
    "print(accuracy_score(bool_test, bool_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAFTLLFATCIARH\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "x = correct_neighborhood.get(0)\n",
    "print(x)\n",
    "x = fsl2.word_to_vector(x)\n",
    "x = np.array([x])\n",
    "k_x = ProbabilisticKernel(x, X_train)\n",
    "print(x)\n",
    "test = probabilisticClassifier.predict(k_x)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de faux négatifs :412\n",
      "Nombre de faux positifs :57\n",
      "Nombre total :1408\n"
     ]
    }
   ],
   "source": [
    "count_fn = 0\n",
    "for i in range(0, len(correct_neighborhood)):\n",
    "    x = np.array([fsl2.word_to_vector(correct_neighborhood.get(i))])\n",
    "    K_x = ProbabilisticKernel(x, X_train)\n",
    "    test = probabilisticClassifier.predict(K_x)\n",
    "    if not(test):\n",
    "        count_fn += 1\n",
    "\n",
    "print(\"Nombre de faux négatifs :\" + str(count_fn))\n",
    "\n",
    "count_fp = 0\n",
    "for i in range(0, len(incorrect_neighborhood)):\n",
    "    try : \n",
    "        x = np.array([fsl2.word_to_vector(incorrect_neighborhood.get(i))])\n",
    "        K_x = ProbabilisticKernel(x, X_train)\n",
    "        test = probabilisticClassifier.predict(K_x)\n",
    "        if test:\n",
    "            count_fp += 1\n",
    "    except:\n",
    "        print(\"Erreur\" + str(i))\n",
    "        #print(incorrect_neighborhood.get(i))\n",
    "\n",
    "\n",
    "print(\"Nombre de faux positifs :\" + str(count_fp))\n",
    "print(\"Nombre total :\" + str(len(correct_neighborhood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developpement et entrainement du modèle similarité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1126, 390)\n",
      "(390,)\n",
      "Test similarity kernel\n",
      "[[3.13913279e-17 9.00171313e+01 2.44691932e+02 ... 3.67879441e-01\n",
      "  2.23130160e-01 1.21824940e+01]\n",
      " [9.00171313e+01 8.53304763e-17 1.00000000e+00 ... 2.41549528e+07\n",
      "  2.23130160e-01 1.09663316e+03]\n",
      " [2.44691932e+02 1.00000000e+00 1.87952882e-12 ... 3.31154520e+01\n",
      "  4.97870684e-02 3.31154520e+01]\n",
      " ...\n",
      " [3.67879441e-01 2.41549528e+07 3.31154520e+01 ... 2.57675711e-18\n",
      "  7.38905610e+00 7.38905610e+00]\n",
      " [2.23130160e-01 2.23130160e-01 4.97870684e-02 ... 7.38905610e+00\n",
      "  1.71390843e-15 5.45981500e+01]\n",
      " [1.21824940e+01 1.09663316e+03 3.31154520e+01 ... 7.38905610e+00\n",
      "  5.45981500e+01 1.73620528e-20]]\n",
      "581\n",
      "1126\n",
      "Test rbf sim kernel\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_all)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest rbf sim kernel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mRBF_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n",
      "Cell \u001b[0;32mIn[26], line 42\u001b[0m, in \u001b[0;36mRBF_kernel\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Calculate the kernel value for each pair of samples\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[0;32m---> 42\u001b[0m     X_str \u001b[38;5;241m=\u001b[39m \u001b[43mfsl2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_to_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Y):\n\u001b[1;32m     44\u001b[0m         Y_str \u001b[38;5;241m=\u001b[39m fsl2\u001b[38;5;241m.\u001b[39mvector_to_word(y)\n",
      "File \u001b[0;32m~/Documents/GitHub/442-2-Protein-cleavage/fonctionsSupervisedLearning2.py:48\u001b[0m, in \u001b[0;36mvector_to_word\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvector_to_word\u001b[39m(vec):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Define a function to decode a vector into a word\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvec\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m26\u001b[39m):\n\u001b[1;32m     49\u001b[0m         word \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform([np\u001b[38;5;241m.\u001b[39margmax(vec[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m26\u001b[39m])])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "X_train, X_test, bool_train, bool_test = test_train_split_random_pos_proba(df_exploitable, n)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_train[0].shape)\n",
    "\n",
    "print(\"Test similarity kernel\")\n",
    "test = RBF_kernel(X_test, X_test)\n",
    "print(test)\n",
    "\n",
    "count_true = 0\n",
    "count_all = 0\n",
    "for t in bool_train:\n",
    "    count_all += 1\n",
    "    if t:\n",
    "        count_true += 1\n",
    "print(count_true)\n",
    "print(count_all)\n",
    "\n",
    "print(\"Test rbf sim kernel\")\n",
    "test = RBF_kernel(X_train[0], X_train[1])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[1 4]\n",
      "Predicted labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom kernel function (e.g., a simple polynomial kernel)\n",
    "def polynomial_kernel(X, Y):\n",
    "    return (1 + np.dot(X, Y.T))**2  # (x*y' + 1)^2\n",
    "\n",
    "# Example data\n",
    "X_train = np.array([[0, 0], [1, 1], [1, 0], [0, 1]])\n",
    "y_train = [0, 1, 1, 0]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "\n",
    "test = polynomial_kernel([X_train[0], X_train[2]], X_train[1])\n",
    "\n",
    "print(test)\n",
    "\n",
    "# Create the SVM with the custom kernel function\n",
    "clf = svm.SVC(kernel=polynomial_kernel)\n",
    "\n",
    "# Train the model using the feature vectors directly\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on new data\n",
    "X_test = np.array([[0, 0], [1, 1]])\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Predicted labels:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39mRBF_kernel)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbool_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m bool_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_score(bool_test, bool_pred))\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/sklearn/svm/_base.py:312\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__Xfit \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    310\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_kernel(X)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[0] should be equal to X.shape[1]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel=RBF_kernel)\n",
    "\n",
    "clf.fit(X_train, bool_train)\n",
    "\n",
    "bool_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(accuracy_score(bool_test, bool_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = correct_neighborhood.get(0)\n",
    "print(x)\n",
    "x = fsl2.word_to_vector(x)\n",
    "x = np.array([x])\n",
    "k_x = ProbabilisticKernel(x, X_train)\n",
    "print(x)\n",
    "test = probabilisticClassifier.predict(k_x)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_fn = 0\n",
    "for i in range(0, len(correct_neighborhood)):\n",
    "    x = np.array([fsl2.word_to_vector(correct_neighborhood.get(i))])\n",
    "    K_x = ProbabilisticKernel(x, X_train)\n",
    "    test = probabilisticClassifier.predict(K_x)\n",
    "    if not(test):\n",
    "        count_fn += 1\n",
    "\n",
    "print(\"Nombre de faux négatifs :\" + str(count_fn))\n",
    "\n",
    "count_fp = 0\n",
    "for i in range(0, len(incorrect_neighborhood)):\n",
    "    try : \n",
    "        x = np.array([fsl2.word_to_vector(incorrect_neighborhood.get(i))])\n",
    "        K_x = ProbabilisticKernel(x, X_train)\n",
    "        test = probabilisticClassifier.predict(K_x)\n",
    "        if test:\n",
    "            count_fp += 1\n",
    "    except:\n",
    "        print(\"Erreur\" + str(i))\n",
    "        #print(incorrect_neighborhood.get(i))\n",
    "\n",
    "\n",
    "print(\"Nombre de faux positifs :\" + str(count_fp))\n",
    "print(\"Nombre total :\" + str(len(correct_neighborhood)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
